{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Script\n",
    "**Applying pre-processing techniques to the dataset**\n",
    "\n",
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Change the directory to dataset directory**\n",
    "\n",
    "**2. While running test script comment out the line below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\University\\FIT3162\\Project\\Fake-News-Detection\\Data Preprocessing\n"
     ]
    }
   ],
   "source": [
    "#cd \"D:\\University\\FIT3162\\Project\\Fake-News-Detection\\Data Preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_columns(df, list_remove):\n",
    "    \"\"\"\n",
    "    Any unwanted columns that are not required in the dataset are removed \n",
    "    by passing a list of columns to the drop function.\n",
    "    param df: dataframe from which the unwanted columns are to be deleted\n",
    "    return: dataframe which now does not contain the unwanted columns\n",
    "    \"\"\"\n",
    "    df = df.drop(list_remove, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicated_posts(df):\n",
    "    \"\"\"\n",
    "    Duplication of posts are eliminated by checking the id of the post or comment.\n",
    "    Since the same id cannot exist twice, such posts are deleted\n",
    "    :param df: dataframe from which duplicated posts must be deleted\n",
    "    :return: dataframe which now contains unique posts or comments only\n",
    "    \"\"\"\n",
    "    # deleting duplicated posts\n",
    "    df = df.drop_duplicates(subset=['id'], keep=False)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_empty_posts(df, subset_list):\n",
    "    \"\"\"\n",
    "    This function deletes all empty posts in the dataset.\n",
    "    Empty posts are NaN\n",
    "    :param filename: name of the file from which the empty posts might be deleted\n",
    "    :param subset list of column names to remove empty values\n",
    "    :return: a dataframe holding the dataset after removing empty posts\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset = subset_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_removed_comments(df):\n",
    "    \"\"\"\n",
    "    This function deletes all removed or deleted comments in the dataset.\n",
    "    Empty posts can be denoted as [deleted] or [removed] or NaN\n",
    "    :param filename: name of the file from which the empty posts might be deleted\n",
    "    :param subset list of column names to remove empty values\n",
    "    :return: a dataframe holding the dataset after removing empty posts\n",
    "    \"\"\"\n",
    "    # Remove all rows which are deleted or removed\n",
    "    df = df.loc[(df['body'] != \"[deleted]\") & (df['body'] != \"[removed]\")]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subreddits(df):\n",
    "    \"\"\"\n",
    "    We are dealing with text only so all sub reddits where images occur those rows\n",
    "    get removed. \n",
    "    param df: dataframe from which the non text rows are to be deleted\n",
    "    return: dataframe which now contains only text\n",
    "    \"\"\"\n",
    "    sub_to_remove = [\"photoshopbattles\", \"pic\" , \"mildlyinteresting\", \"fakealbumcovers\", \"propagandaposters\",\n",
    "                 \"misleadingthumbnails\", \"pareidolia\", \"psbattle_artwork\", \"fakehistoryporn\"]\n",
    "    df = df[~df['subreddit'].isin(sub_to_remove)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Date from UTC to Datetime\n",
    "def get_date(created):\n",
    "    \"\"\"\n",
    "    Gets the correct date in date time format from timestamp format\n",
    "    param: the timestamp date\n",
    "    return: the converted date in date time format\n",
    "    \"\"\"\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def change_date(df):\n",
    "    \"\"\"\n",
    "    Apply the get_date function to the created_utc column\n",
    "    param: dataframe from which the time stamp is to be converted\n",
    "    return: the converted date frame in date time format\n",
    "    \"\"\"\n",
    "    df[\"created_utc\"] = df[\"created_utc\"].apply(get_date)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts(post_df, min_comments):\n",
    "    \"\"\"\n",
    "    Filter the num_comments according to the param passed for min_comments\n",
    "    param: dataframe from which the comments will be filtered, the minimum amount of comments\n",
    "    return: the filtered dataframe where num_comments are greater than specified\n",
    "    \"\"\"\n",
    "    post_df = (post_df.loc[post_df['num_comments'] >= min_comments])\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_comments(df, ids):\n",
    "    \"\"\"\n",
    "    Shows only the comments that are matched with the passed list of ids\n",
    "    param: dataframe of comments, list of ids that are to be filtered for\n",
    "    return: the dataframe comments showed only for the passed in ids\n",
    "    \"\"\"\n",
    "    df = df[df['submission_id'].isin(ids)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_post_dataset():\n",
    "    \"\"\"\n",
    "    Reads the post dataset and applies concat to the test and train tsv files\n",
    "    return: the converted post dataset\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(\"datasetv2.0/train.tsv\", sep='\\t')\n",
    "    test_df = pd.read_csv(\"datasetv2.0/test_public.tsv\", sep='\\t')\n",
    "    validate_df = pd.read_csv(\"datasetv2.0/validate.tsv\", sep='\\t')\n",
    "    post_df = pd.concat([train_df, test_df], axis=0)\n",
    "    post_df = pd.concat([post_df, validate_df], axis=0)\n",
    "    del train_df\n",
    "    del test_df\n",
    "    del validate_df\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comment_dataset():\n",
    "    \"\"\"\n",
    "    Read all comment files in the directory \n",
    "    return: the converted comments dataset\n",
    "    \"\"\"\n",
    "    comment_df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('comments', \"comment*.csv\"))))\n",
    "    comment_df = comment_df.reset_index(drop = True)\n",
    "    return comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main functions that Pre-procesess the posts and comments by applying all the above functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_posts():\n",
    "    \"\"\"\n",
    "    Apply all post functions to dataset for precprocessing\n",
    "    return: the pre-processed post dataset\n",
    "    \"\"\"\n",
    "    post_df = read_post_dataset()\n",
    "    remove_col = ['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1', 'hasImage',\n",
    "       'image_url', 'linked_submission_id', '3_way_label', '6_way_label', 'author']\n",
    "    post_df = remove_unwanted_columns(post_df, remove_col)\n",
    "    post_df = delete_empty_posts(post_df, subset_list=['title'])\n",
    "    post_df = delete_duplicated_posts(post_df)\n",
    "    post_df = remove_subreddits(post_df)\n",
    "    post_df = filter_posts(post_df, 20)\n",
    "    post_df = change_date(post_df)\n",
    "    post_df = post_df.reset_index(drop = True)\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comments(ids):\n",
    "    \"\"\"\n",
    "    Apply all the comment functions for preprocessing\n",
    "    param: the ids for which the comments are to be shown\n",
    "    return: the pre-processed comments dataset\n",
    "    \"\"\"\n",
    "    comment_df = read_comment_dataset()\n",
    "    comment_df = delete_removed_comments(comment_df)\n",
    "    comment_df = delete_empty_posts(comment_df, ['submission_id', 'body'])\n",
    "    comment_df = remove_unwanted_columns(comment_df, ['Column1'])\n",
    "    comment_df = filter_comments(comment_df, ids)\n",
    "    comment_df = comment_df.reset_index(drop = True)\n",
    "    return comment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main functions are called here and the dataset is exported to csv format in the directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Data Preprocessing\")\n",
    "    post_df = preprocess_posts()\n",
    "    ids = post_df.id.unique()\n",
    "    comment_df = preprocess_comments(ids)\n",
    "    post_df.to_csv(\"cleaned_df.csv\", encoding='utf-8-sig')\n",
    "    comment_df.to_csv(\"cleaned_comments.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
